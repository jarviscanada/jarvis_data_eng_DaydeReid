{"paragraphs":[{"text":"%md\n### How to interact with Spark\n\nTo start a Spark job (either single JVM or distributed mode), we can simply execute `bin/spark-shell` cmd which will launch JVM which has the Spark Job running. The entry point of the Spark Job (JVM) is called SparkSession which allows user/application interact with the Spark Job. For instance, you can create RDDs through SparkSession from a exiting Scala collection (e.g. Array, List, Set) or from a data source (e.g. read text from HDFS to RDD)\n\n> Compare SparkContext and Spark Sessionhttps://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\n\nAlternatively, we can use Zeppelin to create a Spark Job and Zeppline will automatically create a SparkSession (`spark`) and a SparkContext (`sc`) for you.\n","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to interact with Spark</h3>\n<p>To start a Spark job (either single JVM or distributed mode), we can simply execute <code>bin/spark-shell</code> cmd which will launch JVM which has the Spark Job running. The entry point of the Spark Job (JVM) is called SparkSession which allows user/application interact with the Spark Job. For instance, you can create RDDs through SparkSession from a exiting Scala collection (e.g. Array, List, Set) or from a data source (e.g. read text from HDFS to RDD)</p>\n<blockquote>\n  <p>Compare SparkContext and Spark <a href=\"Sessionhttps://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\">Sessionhttps://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html</a> <a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession</a> <a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext</a></p>\n</blockquote>\n<p>Alternatively, we can use Zeppelin to create a Spark Job and Zeppline will automatically create a SparkSession (<code>spark</code>) and a SparkContext (<code>sc</code>) for you.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171257_677591144","id":"20190921-014743_1530188134","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:446"},{"text":"%spark\n\n//Spark session and sparkContext are loaded automatically\nprintln(spark.version.to)\nprintln(spark)\n\n//The following two lines point to the same SparkContext@2a695829 where @2a695829 is the memory address\nprintln(spark.sparkContext)\nprintln(sc)\n","user":"anonymous","dateUpdated":"2020-08-10T13:02:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"2.3.4\norg.apache.spark.sql.SparkSession@296aa4f3\norg.apache.spark.SparkContext@19754ab8\norg.apache.spark.SparkContext@19754ab8\n"}]},"apps":[],"jobName":"paragraph_1596811171267_-1852383063","id":"20190921-013657_404311467","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T13:02:34+0000","dateFinished":"2020-08-10T13:02:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:447"},{"text":"%md","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1596811171269_17300170","id":"20190922-220218_788870347","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:448"},{"text":"%md\n### Creating RDDs from Scala collections\n\nWe can use `sc.parallelize` method to create RDDs from Scala collections\n(Note: `parallelize` is not available in the `SparkSession`. However, you can use `SparkSession.sparkContext.parallelize` instead)\n\nhttps://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Scala collections</h3>\n<p>We can use <code>sc.parallelize</code> method to create RDDs from Scala collections<br/>(Note: <code>parallelize</code> is not available in the <code>SparkSession</code>. However, you can use <code>SparkSession.sparkContext.parallelize</code> instead)</p>\n<p><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171274_39547216","id":"20190921-022812_325072599","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:449"},{"text":"%spark\n//Create RDDs from Scala collections\nval lsRdd = sc.parallelize(List(1,2,3,4,5))\n\n//number of items in lsRdd\nval count = lsRdd.count\n\n//first element in lsRdd\nval firstE = lsRdd.first\n\n//Number of partitions\nval partitionsNum = lsRdd.partitions.length\n\n//Manipulating lsRDD\nval dupRdd = lsRdd.flatMap(i => List.fill(i)(i))\nval dupArray = dupRdd.collect\nval evens = dupRdd.filter(_%2 == 0).collect","user":"anonymous","dateUpdated":"2020-08-10T13:02:37+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lsRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[45] at parallelize at <console>:32\ncount: Long = 5\nfirstE: Int = 1\npartitionsNum: Int = 2\ndupRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[46] at flatMap at <console>:44\ndupArray: Array[Int] = Array(1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5)\nevens: Array[Int] = Array(2, 2, 4, 4, 4, 4)\n"}]},"apps":[],"jobName":"paragraph_1596811171274_-419901649","id":"20190921-020350_225494359","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T13:02:37+0000","dateFinished":"2020-08-10T13:02:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:450"},{"text":"%md","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1596811171275_94727426","id":"20190922-220230_613999600","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:451"},{"text":"%md\n### Creating RDDs from Data source\n\n- `ssh` to dataproc master node\n- Download `online-retail-dataset.txt` dataset [link](https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/retail-data/all/online-retail-dataset.csv)\n- Upload `online-retail-dataset.txt` to a HDFS location (e.g. hdfs dfs -put ...)\n- Inspect the dataset using spark RDD (see below Spark code)\n- Discuss why there are some lines that have more than 8 columns (hint: csv format)\n- Discuss some possible solutions ","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Data source</h3>\n<ul>\n  <li><code>ssh</code> to dataproc master node</li>\n  <li>Download <code>online-retail-dataset.txt</code> dataset <a href=\"https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/retail-data/all/online-retail-dataset.csv\">link</a></li>\n  <li>Upload <code>online-retail-dataset.txt</code> to a HDFS location (e.g. hdfs dfs -put &hellip;)</li>\n  <li>Inspect the dataset using spark RDD (see below Spark code)</li>\n  <li>Discuss why there are some lines that have more than 8 columns (hint: csv format)</li>\n  <li>Discuss some possible solutions</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171276_-1471816587","id":"20190920-182511_1653833929","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:452"},{"text":"%spark\nval retailRDD = sc.textFile(\"hdfs:///user/reiddayde/online-retail-dataset.csv\")\n\n//count number of elements in the RDD\nretailRDD.count\n\n//understand what does each element look like in RDD\nval firstE = retailRDD.first()\n\n//find out does withReplacement mean from the scal doc https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\nretailRDD.takeSample(false, 10, 1).foreach(println)","user":"anonymous","dateUpdated":"2020-08-10T13:02:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"552290,21430,SET/3 RED GINGHAM ROSE STORAGE BOX,1,5/8/2011 13:32,3.75,16007,United Kingdom\n578349,22636,CHILDS BREAKFAST SET CIRCUS PARADE,2,11/24/2011 9:50,8.5,14539,United Kingdom\n537666,84917,WHITE HAND TOWEL WITH BUTTERFLY,1,12/7/2010 18:36,4.21,,United Kingdom\n547021,20749,ASSORTED COLOUR MINI CASES,2,3/18/2011 15:43,7.95,13046,United Kingdom\n553718,35809A,ENAMEL PINK TEA CONTAINER,1,5/18/2011 16:14,2.46,,United Kingdom\n557466,21242,RED RETROSPOT PLATE ,8,6/20/2011 13:08,1.69,13815,Germany\n567160,21218,RED SPOTTY BISCUIT TIN,1,9/18/2011 10:35,3.75,14562,United Kingdom\n548975,17003,BROCADE RING PURSE ,108,4/5/2011 11:47,0.29,17596,United Kingdom\n559338,85086A,CANDY SPOT HEART DECORATION,1,7/7/2011 16:30,0.83,,United Kingdom\n546769,22499,WOODEN UNION JACK BUNTING,3,3/16/2011 14:57,5.95,17504,United Kingdom\nretailRDD: org.apache.spark.rdd.RDD[String] = hdfs:///user/reiddayde/online-retail-dataset.csv MapPartitionsRDD[49] at textFile at <console>:31\nfirstE: String = InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\n"}]},"apps":[],"jobName":"paragraph_1596811171276_1616978840","id":"20190920-182724_1961848616","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T13:02:41+0000","dateFinished":"2020-08-10T13:02:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:453"},{"text":"%md\n","user":"anonymous","dateUpdated":"2020-08-07T15:51:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1596811171277_-636076907","id":"20190922-220256_1973670371","dateCreated":"2020-08-07T14:39:31+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:454"},{"text":"%md\n### CSV format issue\nThere are some lines which have more than 8 columns. This is due to the .csv format, which divides columns using commas. The rows with more than 8 columns all have an aditional comma within the Description column that is causing an additional divide.\n\nIn Hive, this issue was solved using a different SerDe to process the data. It is possible that Spark has something similar that could be used to resolve the issue. If not, one could always manually remove the extra commas found in each row.","user":"anonymous","dateUpdated":"2020-08-07T15:34:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>CSV format issue</h3>\n<p>There are some lines which have more than 8 columns. This is due to the .csv format, which divides columns using commas. The rows with more than 8 columns all have an aditional comma within the Description column that is causing an additional divide.</p>\n<p>In Hive, this issue was solved using a different SerDe to process the data. It is possible that Spark has something similar that could be used to resolve the issue. If not, one could always manually remove the extra commas found in each row.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171277_-1305488081","id":"20190921-023538_989684097","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-07T15:34:21+0000","dateFinished":"2020-08-07T15:34:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:455"},{"text":"%spark\n//Check CSV format\nval splitRdd = retailRDD.map(s => s.split(\",\"))\n\n//Some lines have more than 8 column which indicates a format issue\nval samples = splitRdd.map(arr => arr.length).takeSample(false,15, 11)\n\n//find out how many lines have more than 8 cols\nval lenArrRdd = splitRdd.map(arr => (arr.length, arr))\nlenArrRdd.filter(_._1 != 8).take(3).foreach({case(count, cols) => println(count + \":\" + cols.mkString(\"||\"))})\n","user":"anonymous","dateUpdated":"2020-08-10T13:02:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"9:536381||82567||\"AIRLINE LOUNGE||METAL SIGN\"||2||12/1/2010 9:41||2.1||15311||United Kingdom\n9:536394||21506||\"FANCY FONT BIRTHDAY CARD|| \"||24||12/1/2010 10:39||0.42||13408||United Kingdom\n9:536520||22760||\"TRAY|| BREAKFAST IN BED\"||1||12/1/2010 12:43||12.75||14729||United Kingdom\nsplitRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[51] at map at <console>:39\nsamples: Array[Int] = Array(8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8)\nlenArrRdd: org.apache.spark.rdd.RDD[(Int, Array[String])] = MapPartitionsRDD[54] at map at <console>:45\n"}]},"apps":[],"jobName":"paragraph_1596811171278_336606966","id":"20190921-023311_1509488233","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T13:02:45+0000","dateFinished":"2020-08-10T13:02:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:456"},{"text":"%md\n### Pre-process dataset\n\nWe need to deal with fields that contain commas. e.g. `123,\"seond, field\",\"third field\"`. We have seen this csv format issue in Hive, and we solved it using `OpenCSV` SerDe. In this practice, we will simply remove commas between double quotes and then remove all double quotes.\n\n- Removing commas in `Description` field (e.g. \"Apple, Inc\" => \"Apple Inc\")<br>`awk -F'\"' -v OFS='' '{ for (i=2; i<=NF; i+=2) gsub(\",\", \"\", $i) } 1' online-retail-dataset.txt`\n- Remove all double double quotes<br>`sed 's/\"//g' online-retail-dataset.txt`\n- output file: `online-retail-dataset_clean.txt`\n\n### Move file to HDFS\nMove `online-retail-dataset_clean.txt` to HDFS\n\n### Spark RDD Cache\n- https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type","user":"anonymous","dateUpdated":"2020-08-07T15:35:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pre-process dataset</h3>\n<p>We need to deal with fields that contain commas. e.g. <code>123,&quot;seond, field&quot;,&quot;third field&quot;</code>. We have seen this csv format issue in Hive, and we solved it using <code>OpenCSV</code> SerDe. In this practice, we will simply remove commas between double quotes and then remove all double quotes.</p>\n<ul>\n  <li>Removing commas in <code>Description</code> field (e.g. &ldquo;Apple, Inc&rdquo; =&gt; &ldquo;Apple Inc&rdquo;)<br><code>awk -F&#39;&quot;&#39; -v OFS=&#39;&#39; &#39;{ for (i=2; i&lt;=NF; i+=2) gsub(&quot;,&quot;, &quot;&quot;, $i) } 1&#39; online-retail-dataset.txt</code></li>\n  <li>Remove all double double quotes<br><code>sed &#39;s/&quot;//g&#39; online-retail-dataset.txt</code></li>\n  <li>output file: <code>online-retail-dataset_clean.txt</code></li>\n</ul>\n<h3>Move file to HDFS</h3>\n<p>Move <code>online-retail-dataset_clean.txt</code> to HDFS</p>\n<h3>Spark RDD Cache</h3>\n<ul>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence\">https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence</a></li>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171278_1265383278","id":"20190519-113048_765206384","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-07T15:35:03+0000","dateFinished":"2020-08-07T15:35:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:457"},{"text":"//Load csv file\n//Lazy evaluation\n//val datasetDir = \"/home/centos/dev/jrvs/bootcamp/hadoop/datasets\"\nval filePath = \"hdfs:///user/reiddayde/online-retail-dataset_clean.csv\"\nval retailRDD = sc.textFile(filePath)\n\n//RDD action triggers evaluation (in this case count is an action)\nval count = retailRDD.count\n\n//tip: Use tab key to auto-complete\nval sample3 = retailRDD.takeSample(false, 3, 22)\n\n//Make sure every row has exactly 0 columns\nval longRow = retailRDD.filter(row => row.split(\",\").length != 8 ).count\n\n//Cache RDD since it will be accessed frequently\nretailRDD.cache","user":"anonymous","dateUpdated":"2020-08-10T13:02:51+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filePath: String = hdfs:///user/reiddayde/online-retail-dataset_clean.csv\nretailRDD: org.apache.spark.rdd.RDD[String] = hdfs:///user/reiddayde/online-retail-dataset_clean.csv MapPartitionsRDD[57] at textFile at <console>:36\ncount: Long = 541910\nsample3: Array[String] = Array(569457,22329,ROUND CONTAINER SET OF 5 RETROSPOT,1,10/4/2011 11:29,1.65,14606,United Kingdom, 571265,22530,MAGIC DRAWING SLATE DOLLY GIRL ,2,10/16/2011 11:31,0.42,16674,United Kingdom, 563893,90064B,BLACK VINTAGE  CRYSTAL EARRINGS,1,8/19/2011 17:10,3.75,16330,United Kingdom)\nlongRow: Long = 0\nres31: retailRDD.type = hdfs:///user/reiddayde/online-retail-dataset_clean.csv MapPartitionsRDD[57] at textFile at <console>:36\n"}]},"apps":[],"jobName":"paragraph_1596811171279_-1503575831","id":"20190519-105016_1691323616","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T13:02:51+0000","dateFinished":"2020-08-10T13:02:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:458"},{"text":"//Making some utilities and make your life easier :)\n//val printRddNSamples = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.takeSample(false, n, 22).foreach(println)\n//val printRdd3Samples = (rdd: org.apache.spark.rdd.RDD[_]) => printRddNSamples(rdd, 3)\n//val printRddTopN = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.take(n).foreach(println)\nval bars = \"---------\"\nval printMsg = (msg:String) => println(bars+msg+bars)\n","user":"anonymous","dateUpdated":"2020-08-10T12:38:50+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"bars: String = ---------\nprintMsg: String => Unit = <function1>\n"}]},"apps":[],"jobName":"paragraph_1596811171280_-1593890254","id":"20190519-192640_1954412488","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T12:38:50+0000","dateFinished":"2020-08-10T12:38:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:459"},{"text":"%md\n### Spark RDD Transformations and Actions\n\n- <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#transformations target=\"_blank\">Transformations</a>\n- <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#actions target=\"_blank\">Actions</a>\n- Databrick RDD operations http://bit.ly/30ez9IG\n- Spark: The Definitive Guide Chapter 12 (required) & Chapter 13 (Optional)\n\n#### RDD Actions\n1. Get the first element from `retailRDD`\n2. Get the first 5 elements from `retailRDD` as an array.\n3. Get all elements from `retailRDD` as an array\n4. Get random 5 elements from `retailRDD` as an array\n5. Save all elements from `retailRDD` to local file `hdfs:///tmp/text.txt`\n\nSample outputs:\n```bash\n#1\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n#3\n541909\n#4\n575477,85099F,JUMBO BAG STRAWBERRY,2,11/9/2011 16:14,4.13,,United Kingdom\n574536,22988,SOLDIERS EGG CUP ,5,11/4/2011 14:35,1.25,15707,United Kingdom\n538104,22748,POPPYS PLAYHOUSE KITCHEN,2,12/9/2010 15:16,2.1,17950,United Kingdom\n555150,84991,60 TEATIME FAIRY CAKE CASES,24,5/31/2011 15:53,0.55,,United Kingdom\n550496,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,4/18/2011 15:05,8.29,,United Kingdom\n#5\nedward@jarvis-hadoop-m:~/BigData$ hdfs dfs -ls /tmp\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-17 18:20 /tmp/test.txt\n\n```\n\n","user":"anonymous","dateUpdated":"2020-08-07T16:44:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Spark RDD Transformations and Actions</h3>\n<ul>\n  <li>\n  <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#transformations target=\"_blank\">Transformations</a></li>\n  <li>\n  <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#actions target=\"_blank\">Actions</a></li>\n  <li>Databrick RDD operations <a href=\"http://bit.ly/30ez9IG\">http://bit.ly/30ez9IG</a></li>\n  <li>Spark: The Definitive Guide Chapter 12 (required) &amp; Chapter 13 (Optional)</li>\n</ul>\n<h4>RDD Actions</h4>\n<ol>\n  <li>Get the first element from <code>retailRDD</code></li>\n  <li>Get the first 5 elements from <code>retailRDD</code> as an array.</li>\n  <li>Get all elements from <code>retailRDD</code> as an array</li>\n  <li>Get random 5 elements from <code>retailRDD</code> as an array</li>\n  <li>Save all elements from <code>retailRDD</code> to local file <code>hdfs:///tmp/text.txt</code></li>\n</ol>\n<p>Sample outputs:</p>\n<pre><code class=\"bash\">#1\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n#3\n541909\n#4\n575477,85099F,JUMBO BAG STRAWBERRY,2,11/9/2011 16:14,4.13,,United Kingdom\n574536,22988,SOLDIERS EGG CUP ,5,11/4/2011 14:35,1.25,15707,United Kingdom\n538104,22748,POPPYS PLAYHOUSE KITCHEN,2,12/9/2010 15:16,2.1,17950,United Kingdom\n555150,84991,60 TEATIME FAIRY CAKE CASES,24,5/31/2011 15:53,0.55,,United Kingdom\n550496,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,4/18/2011 15:05,8.29,,United Kingdom\n#5\nedward@jarvis-hadoop-m:~/BigData$ hdfs dfs -ls /tmp\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-17 18:20 /tmp/test.txt\n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171289_-657931737","id":"20190519-115905_2023471169","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-07T16:44:16+0000","dateFinished":"2020-08-07T16:44:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:460"},{"text":"//your solution\nimport sys.process._\n\nprintMsg(\"#1\")\nprintln(retailRDD.take(2)(1))\n\nprintMsg(\"#2\")\nfor (e <- retailRDD.take(6).drop(1)) println(e)\n\nprintMsg(\"#3\")\nprintln(retailRDD.collect.drop(1).size)\n\nprintMsg(\"#4\")\nfor (e <- retailRDD.takeSample(false, 5)) println(e)\n\nprintMsg(\"#5\")\n\"hdfs dfs -rm -r /tmp/text.txt\" !\n\nretailRDD.saveAsTextFile(\"hdfs:///tmp/text.txt\")\n\n\"hdfs dfs -ls /tmp\" !","user":"anonymous","dateUpdated":"2020-08-10T13:03:00+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":402.246,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there were two feature warnings; re-run with -feature for details\n---------#1---------\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n---------#2---------\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n---------#3---------\n541909\n---------#4---------\n541991,21257,VICTORIAN SEWING BOX MEDIUM,2,1/25/2011 9:11,7.95,12417,Belgium\n568667,21791,VINTAGE HEADS AND TAILS CARD GAME ,1,9/28/2011 13:00,1.25,14606,United Kingdom\n555558,20679,EDWARDIAN PARASOL RED,1,6/5/2011 14:06,5.95,14159,United Kingdom\n537351,84596L,BISCUITS SMALL BOWL LIGHT BLUE,1,12/6/2010 12:23,1.25,16898,United Kingdom\n561630,22354,RETROSPOT PADDED SEAT CUSHION,4,7/28/2011 14:16,3.75,14527,United Kingdom\n---------#5---------\nDeleted /tmp/text.txt\nFound 3 items\ndrwxrwxrwt   - hdfs     hadoop          0 2020-07-24 18:46 /tmp/hadoop-yarn\ndrwx-wx-wx   - hive     hadoop          0 2020-07-30 19:04 /tmp/hive\ndrwxr-xr-x   - zeppelin hadoop          0 2020-08-10 13:03 /tmp/text.txt\nimport sys.process._\nres32: Int = 0\n"}]},"apps":[],"jobName":"paragraph_1596811171291_-1264053785","id":"20190519-122034_629713430","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T13:03:00+0000","dateFinished":"2020-08-10T13:03:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:461"},{"text":"%md","user":"anonymous","dateUpdated":"2020-08-07T16:53:09+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1596811171292_593062299","id":"20190922-215221_1578966852","dateCreated":"2020-08-07T14:39:31+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:462"},{"text":"%md\n#### RDD Transformations\nRDD Transformations are lazy eval. You can trigger and verify you RDD with a action (e.g. `printRddNSamples` uses `takeSample` action) \n\n1. Get all sales from \"United Kingdom\" (hint: use filter)\n2. Compare `sample` and `takeSample`\n\nSample outputs:\n```bash\n#1\n495478\n495478\n495478\n#2\nrdd.sample vs rdd.takeSample\n```","user":"anonymous","dateUpdated":"2020-08-07T16:44:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>RDD Transformations</h4>\n<p>RDD Transformations are lazy eval. You can trigger and verify you RDD with a action (e.g. <code>printRddNSamples</code> uses <code>takeSample</code> action) </p>\n<ol>\n  <li>Get all sales from &ldquo;United Kingdom&rdquo; (hint: use filter)</li>\n  <li>Compare <code>sample</code> and <code>takeSample</code></li>\n</ol>\n<p>Sample outputs:</p>\n<pre><code class=\"bash\">#1\n495478\n495478\n495478\n#2\nrdd.sample vs rdd.takeSample\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171292_-1325725664","id":"20190917-181850_863623231","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-07T16:44:36+0000","dateFinished":"2020-08-07T16:44:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:463"},{"text":"//your solution\nprintMsg(\"#1\")\nval ukRDD = retailRDD.filter(row => row.split(\",\")(7) == \"United Kingdom\")\nval ukCount = ukRDD.collect.size\nprintln(ukCount)\n\nprintMsg(\"#2\")\n// takeSample is an action, meaning it occurs right away, and takes a sample of the specified size\nfor (e <- retailRDD.takeSample(false, 5)) println(e)\nprintln(bars+bars)\n// sample is a transformation, meaning it uses lazy evaluation, and takes a fraction of the full dataset as the sample\nval rddSample = retailRDD.sample(false, 0.000008).collect\nfor (e <- rddSample) println(e)\nprintln(\"\\n\")\n","user":"anonymous","dateUpdated":"2020-08-10T13:03:05+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------#1---------\n495478\n---------#2---------\n572552,22246,MAGIC GARDEN FELT GARLAND ,1,10/24/2011 17:07,4.13,14096,United Kingdom\n562528,21989,PACK OF 20 SKULL PAPER NAPKINS,12,8/5/2011 13:52,0.85,12610,Italy\n542602,21672,WHITE SPOT RED CERAMIC DRAWER KNOB,4,1/30/2011 12:09,1.25,15579,United Kingdom\n565442,22452,MEASURING TAPE BABUSHKA PINK,6,9/4/2011 14:09,2.95,12627,Germany\n541430,22423,REGENCY CAKESTAND 3 TIER,16,1/18/2011 9:50,10.95,12356,Portugal\n------------------\n536876,21261,GREEN GOOSE FEATHER CHRISTMAS TREE ,1,12/3/2010 11:36,7.62,,United Kingdom\n549235,22652,TRAVEL SEWING KIT,1,4/7/2011 11:16,1.65,15530,United Kingdom\n550912,22430,ENAMEL WATERING CAN CREAM,4,4/21/2011 13:17,4.95,13267,United Kingdom\n568193,22960,JAM MAKING SET WITH JARS,2,9/25/2011 14:37,4.25,17107,United Kingdom\n568202,20978,36 PENCILS TUBE SKULLS,16,9/25/2011 15:01,1.25,15366,United Kingdom\n568828,23367,SET 12 COLOUR PENCILS SPACEBOY ,16,9/29/2011 11:31,0.65,15175,United Kingdom\n575477,17084N,FAIRY DREAMS INCENSE ,3,11/9/2011 16:14,0.42,,United Kingdom\n576652,23332,IVORY WICKER HEART LARGE,3,11/16/2011 10:39,1.65,17198,United Kingdom\n\n\nukRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[62] at filter at <console>:46\nukCount: Int = 495478\nrddSample: Array[String] = Array(536876,21261,GREEN GOOSE FEATHER CHRISTMAS TREE ,1,12/3/2010 11:36,7.62,,United Kingdom, 549235,22652,TRAVEL SEWING KIT,1,4/7/2011 11:16,1.65,15530,United Kingdom, 550912,22430,ENAMEL WATERING CAN CREAM,4,4/21/2011 13:17,4.95,13267,United Kingdom, 568193,22960,JAM MAKING SET WITH JARS,2,9/25/2011 14:37,4.25,17107,United Kingdom, 568202,20978,36 PENCILS TUBE SKULLS,16,9/25/2011 15:01,1.25,15366,United Kingdom, 568828,23367,SET 12 COLOUR PENCILS SPACEBOY ,16,9/29/2011 11:31,0.65,15175,United Kingdom, 575477,17084N,FAIRY DREAMS INCENSE ,3,11/9/2011 16:14,0.42,,United Kingdom, 576652,23332,IVORY WICKER HEART LARGE,3,11/16/2011 10:39,1.65,17198,Unite..."}]},"apps":[],"jobName":"paragraph_1596811171293_858871454","id":"20190519-124053_1683164197","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T13:03:05+0000","dateFinished":"2020-08-10T13:03:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:464"},{"text":"%md\n### Pair RDD (KeyValue)\nSo far, each element in `retailRDD` is a String. However, many distributed computation paradigms require KV pairs (e.g. MapReduce). Let's construct Pair RDDs from `retailRDD` in order to perform more advanced computations.\n\n**RDD vs PairRDD**:\n\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pair RDD (KeyValue)</h3>\n<p>So far, each element in <code>retailRDD</code> is a String. However, many distributed computation paradigms require KV pairs (e.g. MapReduce). Let&rsquo;s construct Pair RDDs from <code>retailRDD</code> in order to perform more advanced computations.</p>\n<p><strong>RDD vs PairRDD</strong>:</p>\n<ul>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD</a></li>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171293_734816342","id":"20190519-125017_38292448","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:465"},{"text":"%md\n\n#### Questions 1.0\n\nTrnasform each element in `retailRDD` to a key value pair (as a tuple) as following\n\n```\nkey = country\nvalue = amount (e.g. Quantity * UnitPrice)\n```\n\nhint: \n\n- use `rdd.map`\n- Use `row.split(\",\")` to tokenize the row\n- Cast quanitity to int while parsing the row\n- Cast price to double\n\n**Sample output**:\n\n```\n//resultRdd.takeSample(false, 3,3)\n//wehere (key=country, value=amount) for each tuple\nres405: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n```\n\n","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 1.0</h4>\n<p>Trnasform each element in <code>retailRDD</code> to a key value pair (as a tuple) as following</p>\n<pre><code>key = country\nvalue = amount (e.g. Quantity * UnitPrice)\n</code></pre>\n<p>hint: </p>\n<ul>\n  <li>use <code>rdd.map</code></li>\n  <li>Use <code>row.split(&quot;,&quot;)</code> to tokenize the row</li>\n  <li>Cast quanitity to int while parsing the row</li>\n  <li>Cast price to double</li>\n</ul>\n<p><strong>Sample output</strong>:</p>\n<pre><code>//resultRdd.takeSample(false, 3,3)\n//wehere (key=country, value=amount) for each tuple\nres405: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171294_-1890641628","id":"20190519-195132_1947538683","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:466"},{"text":"//Question 1.0 solution\nval parseKeyValue = (row: String ) => {\n    val tokens = row.split(\",\")\n    val country= tokens.last\n    val quantity = tokens(3)\n    val unitPrice = tokens(5)\n    val amount = quantity.toInt * unitPrice.toDouble\n    (country, amount)\n}\n\n\nval header = retailRDD.first\nval ctryRdd = retailRDD.filter(row => row != header).map(parseKeyValue)\nctryRdd.takeSample(false, 3,3)","user":"anonymous","dateUpdated":"2020-08-10T13:03:13+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":822,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"parseKeyValue: String => (String, Double) = <function1>\nheader: String = InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\nctryRdd: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[66] at map at <console>:53\nres34: Array[(String, Double)] = Array((Germany,19.799999999999997), (United Kingdom,8.26), (United Kingdom,4.13))\n"}]},"apps":[],"jobName":"paragraph_1596811171294_674126281","id":"20190519-125921_348001552","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T13:03:13+0000","dateFinished":"2020-08-10T13:03:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:467"},{"user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1596811171299_504703701","id":"20190922-215540_2030793994","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:468"},{"text":"%md\n\n#### Questions 1.1\n\nCalculate total sales amount for each country and sort in descending order\n\n```sql\nSELECT country, sum(Quantity * UnitPrice) as total_sales\nFROM retail\nGROUP BY country\nORDER BY total_sales\n```\n\n**Sample output**\n\n```bash\n//resultRdd.take(3)\n//key=country, value=total_sales\nres415: Array[(String, Double)] = Array((France,197403.90000000026), (Malta,2505.470000000001), (Greece,4710.5199999999995))\n```\n\nHints:\n\n- implement `group by` with `rdd.reduceByKey` [doc](http://bit.ly/30fJHHs)\n- implement `order by` with `rdd.sortBy`","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 1.1</h4>\n<p>Calculate total sales amount for each country and sort in descending order</p>\n<pre><code class=\"sql\">SELECT country, sum(Quantity * UnitPrice) as total_sales\nFROM retail\nGROUP BY country\nORDER BY total_sales\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"bash\">//resultRdd.take(3)\n//key=country, value=total_sales\nres415: Array[(String, Double)] = Array((France,197403.90000000026), (Malta,2505.470000000001), (Greece,4710.5199999999995))\n</code></pre>\n<p>Hints:</p>\n<ul>\n  <li>implement <code>group by</code> with <code>rdd.reduceByKey</code> <a href=\"http://bit.ly/30fJHHs\">doc</a></li>\n  <li>implement <code>order by</code> with <code>rdd.sortBy</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171299_-1644521019","id":"20190519-195238_1235609517","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:469"},{"text":"%spark\n//your solution\nval totalSalesRdd = ctryRdd.reduceByKey((sum, i) => sum + i)\nval sortedSalesRdd = totalSalesRdd.sortBy(key => key)\nfor (e <- sortedSalesRdd.collect) println(e)","user":"anonymous","dateUpdated":"2020-08-10T13:03:17+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(Australia,137077.2699999999)\n(Austria,10154.319999999996)\n(Bahrain,548.4)\n(Belgium,40910.95999999998)\n(Brazil,1143.6000000000001)\n(Canada,3666.380000000001)\n(Channel Islands,20086.289999999983)\n(Cyprus,12946.289999999994)\n(Czech Republic,707.72)\n(Denmark,18768.139999999992)\n(EIRE,263276.81999999884)\n(European Community,1291.7499999999998)\n(Finland,22326.74)\n(France,197403.90000000026)\n(Germany,221698.21000000017)\n(Greece,4710.5199999999995)\n(Hong Kong,10117.039999999997)\n(Iceland,4310.000000000001)\n(Israel,7907.82)\n(Italy,16890.509999999987)\n(Japan,35340.619999999995)\n(Lebanon,1693.8800000000003)\n(Lithuania,1661.06)\n(Malta,2505.470000000001)\n(Netherlands,284661.539999999)\n(Norway,35163.46000000001)\n(Poland,7213.139999999999)\n(Portugal,29367.019999999953)\n(RSA,1002.3099999999998)\n(Saudi Arabia,131.17)\n(Singapore,9120.390000000001)\n(Spain,54774.5800000002)\n(Sweden,36595.90999999999)\n(Switzerland,56385.35000000011)\n(USA,1730.9200000000003)\n(United Arab Emirates,1902.2800000000002)\n(United Kingdom,8187806.363998696)\n(Unspecified,4749.789999999998)\ntotalSalesRdd: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[68] at reduceByKey at <console>:49\nsortedSalesRdd: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[73] at sortBy at <console>:50\n"}]},"apps":[],"jobName":"paragraph_1596811171300_1181256716","id":"20190519-195236_1582695577","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T13:03:17+0000","dateFinished":"2020-08-10T13:03:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:470"},{"user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1596811171300_-1567314705","id":"20190922-215553_1555963294","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471"},{"text":"%md\n\n#### Questions 2.0\n\nLet's assume CustomerID is auto-increment. For each country, find the earliest registered customer (e.g. smallest CustomerID)\n\n```sql\nSELECT min(CustomerID), Country\nFROM retail\nGROUP BY Country\n```\n\n**Sample output**\n\n```\n//resultRdd.collect\nArray[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n```\n\n**hints:**\n\n- Generate a new KV pair RDD `(country, id)`\n- use `rdd.reduceByKey` find the smallest\n- ID must be a numric number\n","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 2.0</h4>\n<p>Let&rsquo;s assume CustomerID is auto-increment. For each country, find the earliest registered customer (e.g. smallest CustomerID)</p>\n<pre><code class=\"sql\">SELECT min(CustomerID), Country\nFROM retail\nGROUP BY Country\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code>//resultRdd.collect\nArray[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n</code></pre>\n<p><strong>hints:</strong></p>\n<ul>\n  <li>Generate a new KV pair RDD <code>(country, id)</code></li>\n  <li>use <code>rdd.reduceByKey</code> find the smallest</li>\n  <li>ID must be a numric number</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171302_1893168925","id":"20190519-195157_1405617071","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:472"},{"text":"%spark\n//your solution\nval parseKeyValue = (row: String ) => {\n    val tokens = row.split(\",\")\n    val country= tokens.last\n    val id = tokens(6)\n    (country, id)\n}\n\n\nval header = retailRDD.first\nval idRdd = retailRDD.filter(row => row != header).map(parseKeyValue)\nval smallestIdRdd = idRdd.reduceByKey((smallest, i) => {\n    try {\n        i.toInt\n        if (i < smallest) i else smallest\n    } catch {\n        case e: NumberFormatException => smallest\n    }\n})\nfor (e <- smallestIdRdd.collect) println(e)","user":"anonymous","dateUpdated":"2020-08-10T13:03:23+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(Australia,12386)\n(Portugal,12356)\n(United Kingdom,)\n(Brazil,12769)\n(Canada,15388)\n(Japan,12753)\n(Cyprus,12359)\n(European Community,15108)\n(Finland,12348)\n(Iceland,12347)\n(Netherlands,12759)\n(Singapore,12744)\n(Sweden,12483)\n(RSA,12446)\n(Norway,12350)\n(Denmark,12367)\n(Poland,12576)\n(Israel,12512)\n(Saudi Arabia,12565)\n(Belgium,12361)\n(Lithuania,15332)\n(Greece,12478)\n(Italy,12349)\n(France,12413)\n(Switzerland,12357)\n(Spain,12354)\n(USA,12558)\n(Germany,12426)\n(United Arab Emirates,12739)\n(EIRE,14016)\n(Hong Kong,)\n(Bahrain,)\n(Malta,15480)\n(Unspecified,12363)\n(Channel Islands,14442)\n(Austria,12358)\n(Lebanon,12764)\n(Czech Republic,12781)\nparseKeyValue: String => (String, String) = <function1>\nheader: String = InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\nidRdd: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[75] at map at <console>:55\nsmallestIdRdd: org.apache.spark.rdd.RDD[(String, String)] = ShuffledRDD[76] at reduceByKey at <console>:56\n"}]},"apps":[],"jobName":"paragraph_1596811171303_1034844546","id":"20190519-144439_1578143376","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T13:03:23+0000","dateFinished":"2020-08-10T13:03:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:473"},{"user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1596811171303_-129726363","id":"20190922-215609_1113478409","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:474"},{"text":"%md\n\n### Question 2.1\n\nIt's inconvenient to tokenize each row in every operation. Instead, we count convert `retailRDD[String]` to `itemsRdd:RDD[Item]` where `Item` is a case class as following:\n\n`case class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)`\n\nIn this way, you only parse each row only once here and you can reuse `itemsRdd` in the rest of the questions.\n\n**Sample outputs**\n```scala\n//itemsRdd.take(3)\nres445: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n```\n\n**Hints**:\n\n- write a function to convert a row to a Item, e.g. \n```\nval parseRow2Item: (String) => Item = (row: String) => {\n    //complete body\n}\n```\n- Covert all rows to items, e.g. `val itemsRdd =  retailRDD.map(parseRow2Item)`\n","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Question 2.1</h3>\n<p>It&rsquo;s inconvenient to tokenize each row in every operation. Instead, we count convert <code>retailRDD[String]</code> to <code>itemsRdd:RDD[Item]</code> where <code>Item</code> is a case class as following:</p>\n<p><code>case class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)</code></p>\n<p>In this way, you only parse each row only once here and you can reuse <code>itemsRdd</code> in the rest of the questions.</p>\n<p><strong>Sample outputs</strong></p>\n<pre><code class=\"scala\">//itemsRdd.take(3)\nres445: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n</code></pre>\n<p><strong>Hints</strong>:</p>\n<ul>\n  <li>\n  <p>write a function to convert a row to a Item, e.g. </p>\n  <pre><code>val parseRow2Item: (String) =&gt; Item = (row: String) =&gt; {\n//complete body\n}\n</code></pre></li>\n  <li>Covert all rows to items, e.g. <code>val itemsRdd =  retailRDD.map(parseRow2Item)</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171304_-1453470566","id":"20190921-180308_1963749922","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:475"},{"text":"%spark\n//your solution\ncase class Item(var invoiceNo: String, var stockCode: String, var description: Option[String], var quantity: Int, var invoiceDate: String, var unitPrice: Double, var customerID: Option[Int], var country: String)\n\nval rowToItem: (String) => Item = (row: String) => {\n    val tokens = row.split(\",\")\n    new Item(tokens(0), tokens(1), Some(tokens(2)), tokens(3).toInt, tokens(4), tokens(5).toDouble, if (tokens(6) == \"\") None else Some(tokens(6).toInt), tokens(7))\n}\n\nval header = retailRDD.first\nval itemsRdd = retailRDD.filter(row => row != header).map(rowToItem)","user":"anonymous","dateUpdated":"2020-08-10T14:29:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Item\nrowToItem: String => Item = <function1>\nheader: String = InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\nitemsRdd: org.apache.spark.rdd.RDD[Item] = MapPartitionsRDD[139] at map at <console>:30\n"}]},"apps":[],"jobName":"paragraph_1596811171304_-669973482","id":"20190921-180433_1776305327","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T14:29:46+0000","dateFinished":"2020-08-10T14:29:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:476"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1596811171305_742867432","id":"20190922-215634_337314214","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:477"},{"text":"%md\n### Questions 2.2\n\nRe-implement questions 1.1 & 2.1 using itemsRdd (`RDD[Item]`)","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Questions 2.2</h3>\n<p>Re-implement questions 1.1 &amp; 2.1 using itemsRdd (<code>RDD[Item]</code>)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171305_1288933068","id":"20190922-140917_1244358721","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:478"},{"text":"%spark\n//your solution\nprintMsg(\"Re-implement 1.1 solution\")\nvar groupedItemsRdd = itemsRdd.groupBy(item => item.country)\nvar totalSalesRdd = groupedItemsRdd.map({case (country, itemList) => {\n    val sum = itemList.foldLeft(0.0)((sum, item) => sum + (item.quantity * item.unitPrice))\n    (country, sum)\n}})\nfor (e <- totalSalesRdd.collect) println(e)\nprintMsg(\"Re-implement 2.1 solution\")\nvar smallestIdRdd = groupedItemsRdd.map({case (country, itemList) => {\n    val smallest = itemList.foldLeft(-1)((smallest, item) => \n    if(item.customerID == None){\n        smallest\n    } else if(smallest == -1 || item.customerID.get < smallest){\n        item.customerID.get\n    } else {\n        smallest\n    })\n    (country, smallest)\n}})\nfor (e <- smallestIdRdd.collect) println(e)","user":"anonymous","dateUpdated":"2020-08-10T15:09:49+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------Re-implement 1.1 solution---------\n(Australia,137077.26999999973)\n(Portugal,29367.019999999993)\n(United Kingdom,8187806.364001113)\n(Brazil,1143.6000000000001)\n(Canada,3666.380000000001)\n(Japan,35340.62)\n(Cyprus,12946.289999999999)\n(Finland,22326.73999999997)\n(European Community,1291.75)\n(Netherlands,284661.54000000015)\n(Iceland,4309.999999999997)\n(Singapore,9120.39)\n(Sweden,36595.91)\n(RSA,1002.3099999999998)\n(Norway,35163.46000000004)\n(Denmark,18768.140000000003)\n(Poland,7213.139999999997)\n(Israel,7907.819999999995)\n(Saudi Arabia,131.17)\n(Belgium,40910.960000000225)\n(Lithuania,1661.06)\n(Greece,4710.5199999999995)\n(France,197403.89999999895)\n(Italy,16890.510000000006)\n(Switzerland,56385.35000000003)\n(Spain,54774.57999999993)\n(Germany,221698.20999999822)\n(United Arab Emirates,1902.2800000000004)\n(EIRE,263276.82000000105)\n(USA,1730.9200000000003)\n(Hong Kong,10117.04000000001)\n(Bahrain,548.4)\n(Malta,2505.4700000000016)\n(Unspecified,4749.789999999994)\n(Channel Islands,20086.290000000023)\n(Austria,10154.320000000002)\n(Lebanon,1693.8800000000003)\n(Czech Republic,707.7199999999998)\n---------Re-implement 2.1 solution---------\n(Australia,12386)\n(Portugal,12356)\n(United Kingdom,12346)\n(Brazil,12769)\n(Canada,15388)\n(Japan,12753)\n(Cyprus,12359)\n(European Community,15108)\n(Finland,12348)\n(Iceland,12347)\n(Netherlands,12759)\n(Singapore,12744)\n(Sweden,12483)\n(RSA,12446)\n(Norway,12350)\n(Denmark,12367)\n(Poland,12576)\n(Israel,12512)\n(Saudi Arabia,12565)\n(Belgium,12361)\n(Lithuania,15332)\n(Greece,12478)\n(France,12413)\n(Italy,12349)\n(Switzerland,12357)\n(Spain,12354)\n(Germany,12426)\n(United Arab Emirates,12739)\n(EIRE,14016)\n(USA,12558)\n(Hong Kong,-1)\n(Bahrain,12353)\n(Malta,15480)\n(Unspecified,12363)\n(Channel Islands,14442)\n(Austria,12358)\n(Lebanon,12764)\n(Czech Republic,12781)\ngroupedItemsRdd: org.apache.spark.rdd.RDD[(String, Iterable[Item])] = ShuffledRDD[170] at groupBy at <console>:49\ntotalSalesRdd: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[171] at map at <console>:50\nsmallestIdRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[172] at map at <console>:56\n"}]},"apps":[],"jobName":"paragraph_1596811171306_1521371401","id":"20190922-141025_1178356031","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T15:09:49+0000","dateFinished":"2020-08-10T15:09:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:479"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1596811171306_2101726803","id":"20190922-215642_1393968112","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:480"},{"text":"%md\n#### Questions 3\n\nFind number of customers.\n\n```\nSELECT distinct(customerId)\nFROM retail\n```\n\n**Sample output**\n```scala\n//resultRdd.count\nres458: Long = 4373\n```","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 3</h4>\n<p>Find number of customers.</p>\n<pre><code>SELECT distinct(customerId)\nFROM retail\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"scala\">//resultRdd.count\nres458: Long = 4373\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171307_505307416","id":"20190519-195407_556558321","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:481"},{"text":"%spark\n//your solution\nvar customerCount = itemsRdd.map(_.customerID).distinct.collect.size","user":"anonymous","dateUpdated":"2020-08-10T17:07:30+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"customerCount: Int = 4373\n"}]},"apps":[],"jobName":"paragraph_1596811171307_-716219901","id":"20190519-194831_1486531342","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T17:07:30+0000","dateFinished":"2020-08-10T17:07:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:482"},{"user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1596811171308_-2087590820","id":"20190922-215651_1847295545","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:483"},{"text":"%md\n#### Question 4\n\nFind out the number of invoices/purchases for each customer.\n> Note: a invoiceNo can appear in multiple rows (e.g. a invoice contains more than one item)\n\n```sql\nSELECT CustomerId, count(distinct(invoiceNo)) as purchases\nFROM retail\nGROUP by CustomerId\n```\n\n**Sample output**\n```scala\n//rdd.takeSample(false,3,3)\n//where (key=customerId, value=purchases) for each tuple\nres509: Array[(Any, Int)] = Array((16305,1), (16804,3), (16226,3))\n```\n","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Question 4</h4>\n<p>Find out the number of invoices/purchases for each customer.</p>\n<blockquote>\n  <p>Note: a invoiceNo can appear in multiple rows (e.g. a invoice contains more than one item)</p>\n</blockquote>\n<pre><code class=\"sql\">SELECT CustomerId, count(distinct(invoiceNo)) as purchases\nFROM retail\nGROUP by CustomerId\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"scala\">//rdd.takeSample(false,3,3)\n//where (key=customerId, value=purchases) for each tuple\nres509: Array[(Any, Int)] = Array((16305,1), (16804,3), (16226,3))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171308_-119779823","id":"20190519-195310_661372203","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:484"},{"text":"%spark\n//your solution\nvar customerCount = itemsRdd.groupBy(_.customerID).map({case (id, itemList) => (if (id == None) id else id.get, itemList.map(item => item.invoiceNo).to[Set].size)}).collect","user":"anonymous","dateUpdated":"2020-08-10T17:54:20+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"customerCount: Array[(Any, Int)] = Array((13474,1), (14859,5), (18261,2), (13347,1), (16367,2), (17358,2), (17646,4), (12528,8), (16739,2), (14270,1), (17282,3), (17914,1), (14817,2), (14527,86), (16932,1), (18044,11), (12845,4), (13501,2), (12558,2), (13642,1), (15883,2), (15795,1), (16570,15), (14488,2), (13763,3), (16550,9), (18121,2), (12863,2), (16053,2), (17425,2), (15350,1), (16456,5), (18282,3), (13929,3), (13414,3), (12904,2), (14102,8), (12820,4), (16055,1), (13136,11), (12669,4), (14215,7), (14428,7), (18093,5), (14578,1), (16126,4), (16738,1), (15445,1), (13352,2), (17969,2), (14520,2), (14777,2), (12836,4), (18222,1), (13882,6), (13224,3), (15976,2), (14223,4), (13814,3), (12925,2), (17666,2), (13563,2), (16652,11), (15246,2), (15803,1), (16339,1), (16278,3), (12652,2), (13..."}]},"apps":[],"jobName":"paragraph_1596811171309_-1772095327","id":"20190922-190215_1478690977","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T17:54:20+0000","dateFinished":"2020-08-10T17:54:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:485"},{"text":"%md\n#### SPARK UI\nEvery Spark Job has a web UI for monitroing and debuging purposes.\nGo to GCP > your hadoop cluster > web interfaces > Spark History Server > Spark UI. ","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>SPARK UI</h4>\n<p>Every Spark Job has a web UI for monitroing and debuging purposes.<br/>Go to GCP &gt; your hadoop cluster &gt; web interfaces &gt; Spark History Server &gt; Spark UI.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171310_722625679","id":"20190521-114127_1095254606","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:486"},{"text":"%md\n#### RDD join\n\nPrint `customerId, name, country` using `customers.txt` and  `online-retail-dataset_clean.txt` files\n\n1. Load `datasets/online_retail/customers.txt` to `RDD[Customer]` where `Customer` is a case class as following\n`case class Customer(customerId:Int, name: String)`\n2. Join `RDD[Customer]` with `RDD[item]` on `customerId`\n3. Select uniq `customerId, name, country`\n\n```sql\nSELECT retail.customerID, customer.name, country\nFROM customer LEFT JOIN retail ON customer.id = retail.customerId\n```\n\n**Sample Output**\n```bash\n//resultRdd.take(3)\nres553: Array[(Int, String, String)] = Array((13311,Petra M. Dalton,United Kingdom), (12367,Wade S. Blair,Denmark), (18144,Gage A. Sharpe,United Kingdom))\n```","user":"anonymous","dateUpdated":"2020-08-07T14:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>RDD join</h4>\n<p>Print <code>customerId, name, country</code> using <code>customers.txt</code> and <code>online-retail-dataset_clean.txt</code> files</p>\n<ol>\n  <li>Load <code>datasets/online_retail/customers.txt</code> to <code>RDD[Customer]</code> where <code>Customer</code> is a case class as following<br/><code>case class Customer(customerId:Int, name: String)</code></li>\n  <li>Join <code>RDD[Customer]</code> with <code>RDD[item]</code> on <code>customerId</code></li>\n  <li>Select uniq <code>customerId, name, country</code></li>\n</ol>\n<pre><code class=\"sql\">SELECT retail.customerID, customer.name, country\nFROM customer LEFT JOIN retail ON customer.id = retail.customerId\n</code></pre>\n<p><strong>Sample Output</strong></p>\n<pre><code class=\"bash\">//resultRdd.take(3)\nres553: Array[(Int, String, String)] = Array((13311,Petra M. Dalton,United Kingdom), (12367,Wade S. Blair,Denmark), (18144,Gage A. Sharpe,United Kingdom))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1596811171310_-1445851268","id":"20190519-183851_617743118","dateCreated":"2020-08-07T14:39:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:487"},{"text":"%spark\n\ncase class Customer(customerId:Int, name: String)\n\n//your solution\nval rowToCustomer: (String) => Customer = (row: String) => {\n    val tokens = row.split(\",\")\n    new Customer(tokens(0).toInt, tokens(1))\n}\n\nval filePath = \"hdfs:///user/reiddayde/customers.csv\"\nval customerRdd = sc.textFile(filePath).map(rowToCustomer)","user":"anonymous","dateUpdated":"2020-08-10T18:20:43+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Customer\nrowToCustomer: String => Customer = <function1>\nfilePath: String = hdfs:///user/reiddayde/customers.csv\ncustomerRdd: org.apache.spark.rdd.RDD[Customer] = MapPartitionsRDD[229] at map at <console>:27\n"}]},"apps":[],"jobName":"paragraph_1596811171311_-1534998301","id":"20190922-193513_1093352183","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T18:20:43+0000","dateFinished":"2020-08-10T18:20:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:488"},{"text":"%spark\n//your solution\nval mappedItemRdd = itemsRdd.filter(item => item.customerID != None).map(item => (item.customerID.get, item))\nval mappedCustomerRdd = customerRdd.map(customer => (customer.customerId, customer))\nmappedItemRdd.join(mappedCustomerRdd).map({case (id, (item, customer)) => (id, customer.name, item.country)}).collect.distinct\n","user":"anonymous","dateUpdated":"2020-08-10T18:31:46+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":325.994,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"mappedItemRdd: org.apache.spark.rdd.RDD[(Int, Item)] = MapPartitionsRDD[282] at map at <console>:50\nmappedCustomerRdd: org.apache.spark.rdd.RDD[(Int, Customer)] = MapPartitionsRDD[283] at map at <console>:51\nres122: Array[(Int, String, String)] = Array((15930,\" Philip V. Bradford\",United Kingdom), (17796,\" Alvin V. Ellison\",United Kingdom), (15550,\" Nero D. Walls\",United Kingdom), (17934,\" Uma S. Stephens\",United Kingdom), (17312,\" Azalia D. Gordon\",United Kingdom), (16566,\" Kirby G. Newton\",United Kingdom), (13732,\" Sydnee Q. Finley\",United Kingdom), (14866,\" Kirby G. Newton\",United Kingdom), (17158,\" Benjamin D. Clemons\",United Kingdom), (17080,\" Marcia S. Newton\",United Kingdom), (14882,\" Leslie W. English\",United Kingdom), (13298,\" Indira A. Fowler\",United Kingdom), (13278,\" Arsenio..."}]},"apps":[],"jobName":"paragraph_1596811171312_-1276907005","id":"20190922-193954_207087197","dateCreated":"2020-08-07T14:39:31+0000","dateStarted":"2020-08-10T18:31:46+0000","dateFinished":"2020-08-10T18:31:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:489"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2020-08-10T18:22:52+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1597083772457_1135768718","id":"20200810-182252_1467558640","dateCreated":"2020-08-10T18:22:52+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:490"}],"name":"Jarvis/1-SparkRDD_pub","id":"2FGJAN2M1","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}